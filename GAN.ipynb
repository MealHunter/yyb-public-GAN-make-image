{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e66d0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-29T00:56:58.893Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/5][0/1583]\tLoss_D: 2.0628\tLoss_G: 2.1479\tD(x): 0.2615\tD(G(z)): 0.3178 / 0.1564\n",
      "[0/5][50/1583]\tLoss_D: 0.4100\tLoss_G: 26.3125\tD(x): 0.8370\tD(G(z)): 0.0000 / 0.0000\n",
      "[0/5][100/1583]\tLoss_D: 0.3148\tLoss_G: 12.4799\tD(x): 0.8424\tD(G(z)): 0.0010 / 0.0003\n",
      "[0/5][150/1583]\tLoss_D: 0.3864\tLoss_G: 3.9579\tD(x): 0.9406\tD(G(z)): 0.2408 / 0.0291\n",
      "[0/5][200/1583]\tLoss_D: 0.9077\tLoss_G: 7.7398\tD(x): 0.9453\tD(G(z)): 0.4889 / 0.0012\n",
      "[0/5][250/1583]\tLoss_D: 0.4429\tLoss_G: 4.5202\tD(x): 0.7815\tD(G(z)): 0.1189 / 0.0193\n",
      "[0/5][300/1583]\tLoss_D: 0.3332\tLoss_G: 3.8071\tD(x): 0.8583\tD(G(z)): 0.1321 / 0.0350\n",
      "[0/5][350/1583]\tLoss_D: 0.6267\tLoss_G: 7.1407\tD(x): 0.9042\tD(G(z)): 0.3545 / 0.0021\n",
      "[0/5][400/1583]\tLoss_D: 0.8457\tLoss_G: 7.4747\tD(x): 0.8659\tD(G(z)): 0.4368 / 0.0010\n",
      "[0/5][450/1583]\tLoss_D: 0.4197\tLoss_G: 5.8862\tD(x): 0.8821\tD(G(z)): 0.1877 / 0.0070\n",
      "[0/5][500/1583]\tLoss_D: 0.4592\tLoss_G: 2.2827\tD(x): 0.7583\tD(G(z)): 0.0572 / 0.1715\n",
      "[0/5][550/1583]\tLoss_D: 0.6367\tLoss_G: 6.0893\tD(x): 0.8289\tD(G(z)): 0.2927 / 0.0046\n",
      "[0/5][600/1583]\tLoss_D: 0.4520\tLoss_G: 5.3108\tD(x): 0.7557\tD(G(z)): 0.0553 / 0.0141\n",
      "[0/5][650/1583]\tLoss_D: 0.4993\tLoss_G: 4.0296\tD(x): 0.8126\tD(G(z)): 0.1909 / 0.0303\n",
      "[0/5][700/1583]\tLoss_D: 0.7174\tLoss_G: 3.1790\tD(x): 0.7700\tD(G(z)): 0.2704 / 0.0637\n",
      "[0/5][750/1583]\tLoss_D: 0.7897\tLoss_G: 2.0958\tD(x): 0.5967\tD(G(z)): 0.0884 / 0.2016\n",
      "[0/5][800/1583]\tLoss_D: 0.7778\tLoss_G: 7.8837\tD(x): 0.9525\tD(G(z)): 0.4347 / 0.0010\n",
      "[0/5][850/1583]\tLoss_D: 0.4752\tLoss_G: 4.9906\tD(x): 0.8617\tD(G(z)): 0.2259 / 0.0146\n",
      "[0/5][900/1583]\tLoss_D: 0.2586\tLoss_G: 4.7434\tD(x): 0.8843\tD(G(z)): 0.0926 / 0.0158\n",
      "[0/5][950/1583]\tLoss_D: 1.0974\tLoss_G: 7.7279\tD(x): 0.9278\tD(G(z)): 0.5116 / 0.0015\n",
      "[0/5][1000/1583]\tLoss_D: 0.5762\tLoss_G: 4.0895\tD(x): 0.8254\tD(G(z)): 0.2406 / 0.0277\n",
      "[0/5][1050/1583]\tLoss_D: 0.4624\tLoss_G: 6.1697\tD(x): 0.8912\tD(G(z)): 0.2511 / 0.0038\n",
      "[0/5][1100/1583]\tLoss_D: 0.7173\tLoss_G: 5.7491\tD(x): 0.9132\tD(G(z)): 0.3903 / 0.0092\n",
      "[0/5][1150/1583]\tLoss_D: 0.3230\tLoss_G: 3.6000\tD(x): 0.9486\tD(G(z)): 0.2086 / 0.0432\n",
      "[0/5][1200/1583]\tLoss_D: 0.5316\tLoss_G: 3.3038\tD(x): 0.7821\tD(G(z)): 0.1784 / 0.0673\n",
      "[0/5][1250/1583]\tLoss_D: 0.7553\tLoss_G: 5.9462\tD(x): 0.9570\tD(G(z)): 0.4473 / 0.0053\n",
      "[0/5][1300/1583]\tLoss_D: 0.4818\tLoss_G: 3.2051\tD(x): 0.7143\tD(G(z)): 0.0458 / 0.0674\n",
      "[0/5][1350/1583]\tLoss_D: 0.5337\tLoss_G: 4.7364\tD(x): 0.9352\tD(G(z)): 0.3169 / 0.0225\n",
      "[0/5][1400/1583]\tLoss_D: 1.1142\tLoss_G: 2.5613\tD(x): 0.4625\tD(G(z)): 0.0102 / 0.1226\n",
      "[0/5][1450/1583]\tLoss_D: 0.6904\tLoss_G: 2.2628\tD(x): 0.6132\tD(G(z)): 0.0345 / 0.1605\n",
      "[0/5][1500/1583]\tLoss_D: 0.7868\tLoss_G: 2.8075\tD(x): 0.5893\tD(G(z)): 0.0466 / 0.0967\n",
      "[0/5][1550/1583]\tLoss_D: 0.3279\tLoss_G: 3.5620\tD(x): 0.8485\tD(G(z)): 0.1200 / 0.0469\n",
      "[1/5][0/1583]\tLoss_D: 2.0270\tLoss_G: 4.9770\tD(x): 0.9823\tD(G(z)): 0.7757 / 0.0154\n",
      "[1/5][50/1583]\tLoss_D: 0.3674\tLoss_G: 4.0298\tD(x): 0.8344\tD(G(z)): 0.1249 / 0.0291\n",
      "[1/5][100/1583]\tLoss_D: 0.7327\tLoss_G: 2.0547\tD(x): 0.6302\tD(G(z)): 0.1057 / 0.1862\n",
      "[1/5][150/1583]\tLoss_D: 0.6152\tLoss_G: 2.4841\tD(x): 0.7000\tD(G(z)): 0.1418 / 0.1178\n",
      "[1/5][200/1583]\tLoss_D: 0.9425\tLoss_G: 4.0906\tD(x): 0.9241\tD(G(z)): 0.4720 / 0.0316\n",
      "[1/5][250/1583]\tLoss_D: 0.3122\tLoss_G: 4.2395\tD(x): 0.8751\tD(G(z)): 0.1430 / 0.0240\n",
      "[1/5][300/1583]\tLoss_D: 0.7103\tLoss_G: 5.6004\tD(x): 0.9111\tD(G(z)): 0.4053 / 0.0063\n",
      "[1/5][350/1583]\tLoss_D: 0.5107\tLoss_G: 3.0529\tD(x): 0.7189\tD(G(z)): 0.0858 / 0.0782\n",
      "[1/5][400/1583]\tLoss_D: 0.3996\tLoss_G: 4.3353\tD(x): 0.8944\tD(G(z)): 0.2228 / 0.0197\n",
      "[1/5][450/1583]\tLoss_D: 0.4822\tLoss_G: 3.2492\tD(x): 0.7994\tD(G(z)): 0.1812 / 0.0603\n",
      "[1/5][500/1583]\tLoss_D: 0.7805\tLoss_G: 1.5345\tD(x): 0.5920\tD(G(z)): 0.0678 / 0.2837\n",
      "[1/5][550/1583]\tLoss_D: 0.3603\tLoss_G: 3.5433\tD(x): 0.8474\tD(G(z)): 0.1524 / 0.0416\n",
      "[1/5][600/1583]\tLoss_D: 0.3326\tLoss_G: 4.0250\tD(x): 0.8862\tD(G(z)): 0.1596 / 0.0281\n",
      "[1/5][650/1583]\tLoss_D: 0.5576\tLoss_G: 3.2196\tD(x): 0.8019\tD(G(z)): 0.2365 / 0.0568\n",
      "[1/5][700/1583]\tLoss_D: 0.6585\tLoss_G: 2.3911\tD(x): 0.6535\tD(G(z)): 0.1115 / 0.1398\n",
      "[1/5][750/1583]\tLoss_D: 0.7983\tLoss_G: 5.8054\tD(x): 0.8602\tD(G(z)): 0.4139 / 0.0068\n",
      "[1/5][800/1583]\tLoss_D: 0.4259\tLoss_G: 3.6026\tD(x): 0.8766\tD(G(z)): 0.2262 / 0.0422\n",
      "[1/5][850/1583]\tLoss_D: 0.4860\tLoss_G: 3.7477\tD(x): 0.6976\tD(G(z)): 0.0579 / 0.0405\n",
      "[1/5][900/1583]\tLoss_D: 0.3846\tLoss_G: 4.0257\tD(x): 0.8812\tD(G(z)): 0.1998 / 0.0283\n",
      "[1/5][950/1583]\tLoss_D: 0.4234\tLoss_G: 3.1563\tD(x): 0.8157\tD(G(z)): 0.1517 / 0.0670\n",
      "[1/5][1000/1583]\tLoss_D: 0.5579\tLoss_G: 2.0759\tD(x): 0.7040\tD(G(z)): 0.1164 / 0.1652\n",
      "[1/5][1050/1583]\tLoss_D: 0.4855\tLoss_G: 1.9800\tD(x): 0.7267\tD(G(z)): 0.0927 / 0.1903\n",
      "[1/5][1100/1583]\tLoss_D: 0.5553\tLoss_G: 2.7212\tD(x): 0.7766\tD(G(z)): 0.2136 / 0.0887\n",
      "[1/5][1150/1583]\tLoss_D: 1.6852\tLoss_G: 2.7560\tD(x): 0.2837\tD(G(z)): 0.0075 / 0.1134\n",
      "[1/5][1200/1583]\tLoss_D: 0.7577\tLoss_G: 1.2582\tD(x): 0.5634\tD(G(z)): 0.0494 / 0.3508\n",
      "[1/5][1250/1583]\tLoss_D: 0.4532\tLoss_G: 2.4007\tD(x): 0.7418\tD(G(z)): 0.1004 / 0.1288\n",
      "[1/5][1300/1583]\tLoss_D: 0.5353\tLoss_G: 1.8544\tD(x): 0.7036\tD(G(z)): 0.1217 / 0.1947\n",
      "[1/5][1350/1583]\tLoss_D: 0.5956\tLoss_G: 2.1501\tD(x): 0.6601\tD(G(z)): 0.1060 / 0.1618\n",
      "[1/5][1400/1583]\tLoss_D: 0.4056\tLoss_G: 2.5103\tD(x): 0.7665\tD(G(z)): 0.0953 / 0.1125\n",
      "[1/5][1450/1583]\tLoss_D: 0.6159\tLoss_G: 3.8240\tD(x): 0.8671\tD(G(z)): 0.3296 / 0.0328\n",
      "[1/5][1500/1583]\tLoss_D: 0.6330\tLoss_G: 4.1780\tD(x): 0.9034\tD(G(z)): 0.3701 / 0.0230\n",
      "[1/5][1550/1583]\tLoss_D: 0.8374\tLoss_G: 0.6968\tD(x): 0.5305\tD(G(z)): 0.0791 / 0.5460\n",
      "[2/5][0/1583]\tLoss_D: 0.4645\tLoss_G: 2.8946\tD(x): 0.7688\tD(G(z)): 0.1392 / 0.0788\n",
      "[2/5][50/1583]\tLoss_D: 0.4979\tLoss_G: 3.3861\tD(x): 0.8268\tD(G(z)): 0.2280 / 0.0491\n",
      "[2/5][100/1583]\tLoss_D: 0.5882\tLoss_G: 2.7678\tD(x): 0.7702\tD(G(z)): 0.2407 / 0.0900\n",
      "[2/5][150/1583]\tLoss_D: 0.4704\tLoss_G: 3.1779\tD(x): 0.8543\tD(G(z)): 0.2390 / 0.0566\n",
      "[2/5][200/1583]\tLoss_D: 0.6553\tLoss_G: 3.5290\tD(x): 0.8503\tD(G(z)): 0.3465 / 0.0445\n",
      "[2/5][250/1583]\tLoss_D: 0.5753\tLoss_G: 2.8981\tD(x): 0.8095\tD(G(z)): 0.2648 / 0.0763\n",
      "[2/5][300/1583]\tLoss_D: 0.6480\tLoss_G: 3.5625\tD(x): 0.8535\tD(G(z)): 0.3432 / 0.0382\n",
      "[2/5][350/1583]\tLoss_D: 0.6047\tLoss_G: 2.0087\tD(x): 0.6288\tD(G(z)): 0.0572 / 0.1803\n",
      "[2/5][400/1583]\tLoss_D: 0.4501\tLoss_G: 2.7658\tD(x): 0.7821\tD(G(z)): 0.1577 / 0.0858\n",
      "[2/5][450/1583]\tLoss_D: 1.3833\tLoss_G: 1.9775\tD(x): 0.3607\tD(G(z)): 0.0812 / 0.2152\n",
      "[2/5][500/1583]\tLoss_D: 0.5023\tLoss_G: 2.9532\tD(x): 0.9021\tD(G(z)): 0.2945 / 0.0732\n",
      "[2/5][550/1583]\tLoss_D: 0.5690\tLoss_G: 3.1729\tD(x): 0.8192\tD(G(z)): 0.2754 / 0.0537\n",
      "[2/5][600/1583]\tLoss_D: 0.5281\tLoss_G: 1.9958\tD(x): 0.7679\tD(G(z)): 0.1889 / 0.1673\n",
      "[2/5][650/1583]\tLoss_D: 0.7241\tLoss_G: 3.7640\tD(x): 0.9404\tD(G(z)): 0.4464 / 0.0345\n",
      "[2/5][700/1583]\tLoss_D: 0.8186\tLoss_G: 1.0813\tD(x): 0.6059\tD(G(z)): 0.2130 / 0.3768\n",
      "[2/5][750/1583]\tLoss_D: 0.6594\tLoss_G: 5.1555\tD(x): 0.8892\tD(G(z)): 0.3733 / 0.0094\n",
      "[2/5][800/1583]\tLoss_D: 0.6987\tLoss_G: 1.5025\tD(x): 0.5954\tD(G(z)): 0.0896 / 0.2751\n",
      "[2/5][850/1583]\tLoss_D: 0.7761\tLoss_G: 1.3521\tD(x): 0.5878\tD(G(z)): 0.1274 / 0.3096\n",
      "[2/5][900/1583]\tLoss_D: 1.5404\tLoss_G: 0.8972\tD(x): 0.3050\tD(G(z)): 0.0887 / 0.4715\n",
      "[2/5][950/1583]\tLoss_D: 0.8010\tLoss_G: 1.0745\tD(x): 0.5629\tD(G(z)): 0.1213 / 0.3800\n",
      "[2/5][1000/1583]\tLoss_D: 0.4843\tLoss_G: 2.4688\tD(x): 0.7835\tD(G(z)): 0.1849 / 0.1081\n",
      "[2/5][1050/1583]\tLoss_D: 1.0338\tLoss_G: 1.1807\tD(x): 0.4270\tD(G(z)): 0.0443 / 0.3750\n",
      "[2/5][1100/1583]\tLoss_D: 0.7110\tLoss_G: 3.7685\tD(x): 0.9222\tD(G(z)): 0.4287 / 0.0314\n",
      "[2/5][1150/1583]\tLoss_D: 0.6368\tLoss_G: 2.8146\tD(x): 0.8096\tD(G(z)): 0.3124 / 0.0776\n",
      "[2/5][1200/1583]\tLoss_D: 0.5750\tLoss_G: 2.1895\tD(x): 0.6865\tD(G(z)): 0.1234 / 0.1434\n",
      "[2/5][1250/1583]\tLoss_D: 0.5625\tLoss_G: 2.4885\tD(x): 0.7561\tD(G(z)): 0.2149 / 0.1060\n",
      "[2/5][1300/1583]\tLoss_D: 0.8274\tLoss_G: 1.8261\tD(x): 0.5511\tD(G(z)): 0.1221 / 0.2113\n",
      "[2/5][1350/1583]\tLoss_D: 1.0097\tLoss_G: 1.1300\tD(x): 0.4341\tD(G(z)): 0.0316 / 0.3887\n",
      "[2/5][1400/1583]\tLoss_D: 0.7494\tLoss_G: 4.0470\tD(x): 0.9156\tD(G(z)): 0.4353 / 0.0253\n",
      "[2/5][1450/1583]\tLoss_D: 2.4910\tLoss_G: 5.9982\tD(x): 0.9753\tD(G(z)): 0.8710 / 0.0040\n",
      "[2/5][1500/1583]\tLoss_D: 0.6456\tLoss_G: 1.5039\tD(x): 0.6213\tD(G(z)): 0.0994 / 0.2639\n",
      "[2/5][1550/1583]\tLoss_D: 0.6673\tLoss_G: 2.0463\tD(x): 0.6965\tD(G(z)): 0.2101 / 0.1604\n",
      "[3/5][0/1583]\tLoss_D: 0.6881\tLoss_G: 2.0697\tD(x): 0.7310\tD(G(z)): 0.2621 / 0.1530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/5][50/1583]\tLoss_D: 0.8239\tLoss_G: 2.3033\tD(x): 0.6925\tD(G(z)): 0.3033 / 0.1302\n",
      "[3/5][100/1583]\tLoss_D: 1.0746\tLoss_G: 3.5029\tD(x): 0.9417\tD(G(z)): 0.5771 / 0.0422\n",
      "[3/5][150/1583]\tLoss_D: 0.5580\tLoss_G: 1.7387\tD(x): 0.6693\tD(G(z)): 0.1011 / 0.2121\n",
      "[3/5][200/1583]\tLoss_D: 0.6426\tLoss_G: 2.0172\tD(x): 0.7406\tD(G(z)): 0.2296 / 0.1725\n",
      "[3/5][250/1583]\tLoss_D: 0.5634\tLoss_G: 2.6356\tD(x): 0.8790\tD(G(z)): 0.3205 / 0.0941\n",
      "[3/5][300/1583]\tLoss_D: 0.8718\tLoss_G: 0.9293\tD(x): 0.5053\tD(G(z)): 0.0815 / 0.4471\n",
      "[3/5][350/1583]\tLoss_D: 0.7634\tLoss_G: 1.5057\tD(x): 0.6344\tD(G(z)): 0.1971 / 0.2627\n",
      "[3/5][400/1583]\tLoss_D: 0.7148\tLoss_G: 1.9325\tD(x): 0.5692\tD(G(z)): 0.0744 / 0.1849\n",
      "[3/5][450/1583]\tLoss_D: 0.9165\tLoss_G: 1.0422\tD(x): 0.5343\tD(G(z)): 0.1708 / 0.3936\n",
      "[3/5][500/1583]\tLoss_D: 0.6597\tLoss_G: 2.6161\tD(x): 0.8558\tD(G(z)): 0.3677 / 0.0885\n",
      "[3/5][550/1583]\tLoss_D: 0.8455\tLoss_G: 2.4686\tD(x): 0.7778\tD(G(z)): 0.3950 / 0.1103\n",
      "[3/5][600/1583]\tLoss_D: 0.6756\tLoss_G: 2.8801\tD(x): 0.8362\tD(G(z)): 0.3516 / 0.0743\n",
      "[3/5][650/1583]\tLoss_D: 0.6279\tLoss_G: 2.3814\tD(x): 0.6717\tD(G(z)): 0.1586 / 0.1270\n",
      "[3/5][700/1583]\tLoss_D: 0.9833\tLoss_G: 1.8574\tD(x): 0.4890\tD(G(z)): 0.0999 / 0.2238\n",
      "[3/5][750/1583]\tLoss_D: 0.7592\tLoss_G: 3.5418\tD(x): 0.8986\tD(G(z)): 0.4398 / 0.0401\n",
      "[3/5][800/1583]\tLoss_D: 0.5893\tLoss_G: 2.3181\tD(x): 0.6973\tD(G(z)): 0.1596 / 0.1232\n",
      "[3/5][850/1583]\tLoss_D: 0.7986\tLoss_G: 3.2955\tD(x): 0.8505\tD(G(z)): 0.4291 / 0.0503\n",
      "[3/5][900/1583]\tLoss_D: 0.5185\tLoss_G: 2.1331\tD(x): 0.7786\tD(G(z)): 0.2037 / 0.1561\n",
      "[3/5][950/1583]\tLoss_D: 0.4952\tLoss_G: 2.3888\tD(x): 0.8080\tD(G(z)): 0.2201 / 0.1142\n",
      "[3/5][1000/1583]\tLoss_D: 1.2052\tLoss_G: 1.7992\tD(x): 0.5100\tD(G(z)): 0.2900 / 0.2182\n",
      "[3/5][1050/1583]\tLoss_D: 0.6339\tLoss_G: 1.4169\tD(x): 0.6853\tD(G(z)): 0.1861 / 0.2773\n",
      "[3/5][1100/1583]\tLoss_D: 1.0335\tLoss_G: 1.3279\tD(x): 0.4490\tD(G(z)): 0.1124 / 0.3249\n",
      "[3/5][1150/1583]\tLoss_D: 0.4843\tLoss_G: 3.2848\tD(x): 0.8971\tD(G(z)): 0.2881 / 0.0489\n",
      "[3/5][1200/1583]\tLoss_D: 0.5420\tLoss_G: 2.8090\tD(x): 0.7876\tD(G(z)): 0.2290 / 0.0781\n",
      "[3/5][1250/1583]\tLoss_D: 1.0606\tLoss_G: 0.6653\tD(x): 0.4533\tD(G(z)): 0.1174 / 0.5502\n",
      "[3/5][1300/1583]\tLoss_D: 0.6662\tLoss_G: 1.8659\tD(x): 0.6785\tD(G(z)): 0.1923 / 0.1870\n",
      "[3/5][1350/1583]\tLoss_D: 1.2757\tLoss_G: 3.7812\tD(x): 0.9606\tD(G(z)): 0.6603 / 0.0340\n",
      "[3/5][1400/1583]\tLoss_D: 0.6447\tLoss_G: 3.0953\tD(x): 0.8538\tD(G(z)): 0.3499 / 0.0580\n",
      "[3/5][1450/1583]\tLoss_D: 0.6497\tLoss_G: 3.2988\tD(x): 0.9084\tD(G(z)): 0.3816 / 0.0516\n",
      "[3/5][1500/1583]\tLoss_D: 1.2549\tLoss_G: 1.2627\tD(x): 0.3733\tD(G(z)): 0.0513 / 0.3518\n",
      "[3/5][1550/1583]\tLoss_D: 0.6969\tLoss_G: 1.7259\tD(x): 0.5712\tD(G(z)): 0.0533 / 0.2201\n",
      "[4/5][0/1583]\tLoss_D: 0.5437\tLoss_G: 2.5127\tD(x): 0.7362\tD(G(z)): 0.1709 / 0.1052\n",
      "[4/5][50/1583]\tLoss_D: 0.6726\tLoss_G: 2.1502\tD(x): 0.7444\tD(G(z)): 0.2708 / 0.1454\n",
      "[4/5][100/1583]\tLoss_D: 1.1031\tLoss_G: 4.3017\tD(x): 0.8415\tD(G(z)): 0.5468 / 0.0197\n",
      "[4/5][150/1583]\tLoss_D: 0.5030\tLoss_G: 2.5883\tD(x): 0.8377\tD(G(z)): 0.2440 / 0.1001\n",
      "[4/5][200/1583]\tLoss_D: 1.1669\tLoss_G: 0.9120\tD(x): 0.4263\tD(G(z)): 0.1394 / 0.4738\n",
      "[4/5][250/1583]\tLoss_D: 0.4895\tLoss_G: 2.4054\tD(x): 0.7893\tD(G(z)): 0.1967 / 0.1078\n",
      "[4/5][300/1583]\tLoss_D: 0.4862\tLoss_G: 2.8359\tD(x): 0.8870\tD(G(z)): 0.2830 / 0.0730\n",
      "[4/5][350/1583]\tLoss_D: 1.0387\tLoss_G: 4.1142\tD(x): 0.9046\tD(G(z)): 0.5625 / 0.0255\n",
      "[4/5][400/1583]\tLoss_D: 0.5670\tLoss_G: 2.3124\tD(x): 0.7570\tD(G(z)): 0.2182 / 0.1241\n",
      "[4/5][450/1583]\tLoss_D: 0.4736\tLoss_G: 1.8688\tD(x): 0.7455\tD(G(z)): 0.1379 / 0.1850\n",
      "[4/5][500/1583]\tLoss_D: 0.6541\tLoss_G: 3.6135\tD(x): 0.8936\tD(G(z)): 0.3757 / 0.0368\n",
      "[4/5][550/1583]\tLoss_D: 0.6326\tLoss_G: 1.4620\tD(x): 0.7166\tD(G(z)): 0.2192 / 0.2683\n",
      "[4/5][600/1583]\tLoss_D: 0.7891\tLoss_G: 3.8374\tD(x): 0.9244\tD(G(z)): 0.4490 / 0.0309\n",
      "[4/5][650/1583]\tLoss_D: 0.7032\tLoss_G: 2.7427\tD(x): 0.8872\tD(G(z)): 0.3906 / 0.0891\n",
      "[4/5][700/1583]\tLoss_D: 0.7813\tLoss_G: 1.1129\tD(x): 0.5425\tD(G(z)): 0.0731 / 0.3790\n",
      "[4/5][750/1583]\tLoss_D: 0.4402\tLoss_G: 1.8396\tD(x): 0.7510\tD(G(z)): 0.1141 / 0.1977\n",
      "[4/5][800/1583]\tLoss_D: 0.5389\tLoss_G: 3.1366\tD(x): 0.8846\tD(G(z)): 0.3010 / 0.0594\n",
      "[4/5][850/1583]\tLoss_D: 0.5973\tLoss_G: 2.2367\tD(x): 0.7698\tD(G(z)): 0.2525 / 0.1340\n",
      "[4/5][900/1583]\tLoss_D: 0.9160\tLoss_G: 1.9140\tD(x): 0.5346\tD(G(z)): 0.1512 / 0.1994\n",
      "[4/5][950/1583]\tLoss_D: 0.4920\tLoss_G: 2.3285\tD(x): 0.7777\tD(G(z)): 0.1839 / 0.1289\n",
      "[4/5][1000/1583]\tLoss_D: 0.8775\tLoss_G: 3.7410\tD(x): 0.9160\tD(G(z)): 0.5036 / 0.0317\n",
      "[4/5][1050/1583]\tLoss_D: 1.0737\tLoss_G: 4.7808\tD(x): 0.9478\tD(G(z)): 0.5876 / 0.0120\n",
      "[4/5][1100/1583]\tLoss_D: 0.6715\tLoss_G: 1.5153\tD(x): 0.5909\tD(G(z)): 0.0758 / 0.2753\n",
      "[4/5][1150/1583]\tLoss_D: 0.4638\tLoss_G: 2.8780\tD(x): 0.8531\tD(G(z)): 0.2404 / 0.0698\n",
      "[4/5][1200/1583]\tLoss_D: 0.5029\tLoss_G: 2.1458\tD(x): 0.7859\tD(G(z)): 0.2040 / 0.1405\n",
      "[4/5][1250/1583]\tLoss_D: 1.6170\tLoss_G: 1.7388\tD(x): 0.6545\tD(G(z)): 0.6297 / 0.2137\n",
      "[4/5][1300/1583]\tLoss_D: 0.6719\tLoss_G: 1.9962\tD(x): 0.7068\tD(G(z)): 0.2273 / 0.1652\n",
      "[4/5][1350/1583]\tLoss_D: 0.6501\tLoss_G: 2.9485\tD(x): 0.8677\tD(G(z)): 0.3624 / 0.0675\n",
      "[4/5][1400/1583]\tLoss_D: 0.5044\tLoss_G: 2.7381\tD(x): 0.8193\tD(G(z)): 0.2355 / 0.0826\n",
      "[4/5][1450/1583]\tLoss_D: 0.4861\tLoss_G: 2.5017\tD(x): 0.7643\tD(G(z)): 0.1618 / 0.1003\n",
      "[4/5][1500/1583]\tLoss_D: 2.3639\tLoss_G: 5.5560\tD(x): 0.9914\tD(G(z)): 0.8705 / 0.0068\n",
      "[4/5][1550/1583]\tLoss_D: 0.4641\tLoss_G: 2.5291\tD(x): 0.8588\tD(G(z)): 0.2378 / 0.1022\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# 设置随机算子\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    " \n",
    "# 数据集位置\n",
    "dataroot = \"F:/BaiduNetdiskDownload/celeba\"\n",
    " \n",
    "# dataloader的核数\n",
    "workers = 2\n",
    " \n",
    "# Batch大小\n",
    "batch_size = 128\n",
    " \n",
    "# 图像缩放大小\n",
    "image_size = 64\n",
    " \n",
    "# 图像通道数\n",
    "nc = 3\n",
    " \n",
    "# 隐向量维度\n",
    "nz = 100\n",
    " \n",
    "# 生成器特征维度\n",
    "ngf = 64\n",
    " \n",
    "# 判别器特征维度\n",
    "ndf = 64\n",
    " \n",
    "# 训练轮数\n",
    "num_epochs = 5\n",
    " \n",
    "# 学习率\n",
    "lr = 0.0002\n",
    " \n",
    "# Adam优化器的beta系数\n",
    "beta1 = 0.5\n",
    " \n",
    "# gpu个数\n",
    "ngpu = 1\n",
    " \n",
    "# 加载数据集\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# 创建dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    " \n",
    "# 使用cpu还是gpu\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    " \n",
    "# 初始化权重\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# 生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    " \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# 实例化生成器并初始化权重\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    " \n",
    "# 判别器\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    " \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# 实例化判别器并初始化权重\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    " \n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.BCELoss()\n",
    " \n",
    "# 随机输入噪声\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    " \n",
    "# 真实标签与虚假标签\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    " \n",
    "# 创建优化器\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    " \n",
    "# 开始训练\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    " \n",
    "print(\"Starting Training Loop...\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) 更新D: 最大化 log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # 使用真实标签的batch训练\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    " \n",
    "        # 使用虚假标签的batch训练\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        # 更新D\n",
    "        optimizerD.step()\n",
    " \n",
    "        ############################\n",
    "        # (2) 更新G: 最大化 log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # 更新G\n",
    "        optimizerG.step()\n",
    " \n",
    "        # 输出训练状态\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    " \n",
    "        # 保存每轮loss\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    " \n",
    "        # 记录生成的结果\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    " \n",
    "        iters += 1\n",
    "\n",
    "# loss曲线\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    " \n",
    "\n",
    "# 生成效果图\n",
    "real_batch = next(iter(dataloader))\n",
    " \n",
    "# 真实图像\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    " \n",
    "# 生成的虚假图像\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa8f390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
